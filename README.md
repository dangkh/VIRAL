# VLIF
Pytorch implementation for "Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion" [arxiv](https://arxiv.org/pdf/.pdf)

```
VLIF/
│
├── data/
│   └── <dataset_name>/
│       ├── amazon_description_<dataset_name>_sample.json
│       ├── <dataset_name>_5.json
│       ├── <dataset_name>.inter
│       ├── i_id_mapping.csv
│       ├── meta_<dataset_name>.json
│       └── text_feat.npy         # Text embedding file generated by get_text_feat.py
│
├── src
│	├──get_text_feat.py              # Script to generate text embeddings
│	├──README.md
│   └─requirements.txt
```
## Usage

### 1. Install required libraries

```sh
conda create --name vlif python=3.10
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install git+https://github.com/huggingface/transformers accelerate
pip install transformers accelerate timm einops bitsandbytes --quiet
pip install qwen-vl-utils[decord]==0.0.8
torch geometry
matplotlib
```

### 2. Preprocess

#### Generate text features for images

Vlm generate feature for image, edit src/config.py to change the dataset and vlm model.
```sh
python src/vlm2feat.py
```

Move the generated files to corresponding directory, name of generated file sample is: `amazon_baby_model_qwen_type_plain_descriptions.csv`

Place required files:
```
baby: gdown https://drive.google.com/uc?id=1_WKB112C095iHn8djsGCKYmhrdv0xtVS
```
Samples are provided

Example with the "baby" dataset, using the "title" column to replace nan field:
```sh
python src/get_text_feat.py --dataset=baby --text_column=title --type_prompt=sample
```

Arguments:
- `--dataset`: Name of the dataset (e.g., baby)
- `--text_column`: Name of the column containing text data (e.g., title)
- `--txt_embedding_model` (Not required): Name of the embedding model for text data (e.g., sentence-transformers/all-MiniLM-L6-v2)

The embedding file will be saved at: `data/<dataset_name>/en_image_feat.npy`

#### Generate user co-occurrence graph
Example with the "baby" dataset
```sh
python src/generate-u-u-matrix.py -d=baby
```

Full PID (synergy + redundant) is located at redu2 branch, but we suggest run synergy (master branch) only, that offer stable result and faster :D

### 3. Training
Edit config at src/config.py
```sh
python src/main.py -d=baby
```





## Data
Data could be download from: [Baby/Sports/Clothing](https://drive.google.com/drive/u/4/folders/14uEOz_roDoHV8OB4fq8hXpLIfVQofGbT)  


#### Please consider to cite our paper if this model helps you, thanks:
```
here
```
Result baby: recall@5: 0.0430    recall@10: 0.0705    recall@20: 0.1083    recall@50: 0.1772    ndcg@5: 0.0288    ndcg@10: 0.0378    ndcg@20: 0.0474    ndcg@50: 0.0614    precision@5: 0.0095    precision@10: 0.0078    precision@20: 0.0060    precision@50: 0.0040    map@5: 0.0233    map@10: 0.0269    map@20: 0.0295    map@50: 0.0317


This repo is implemented based on work of DRAGON: https://github.com/hongyurain/DRAGON
